pred <- proj_linpred(var_selection,
xnew=data_stan_df$x_mat,
ynew=data_stan_df$y_obs,
nv = top_n, integrated = TRUE)
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=data_stan_df$y_obs)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection <- varsel(fit, method='forward')
# variables ordered as they enter during the search
var_selection$vind
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.4)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(0, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
n_input = 20
n_sig_input = 5
set.seed(13)
true.beta.vec = NULL
sig.beta.idx <- sample(seq(n_input), n_sig_input)
true.beta.vec <- rnorm(n_input, 0.001, 0.01)
true.beta.vec[sig.beta.idx] <- rnorm(n_sig_input, 14, 3)
true.intercept <- rnorm(1, 0.5, 3)
x.mat = matrix(nrow = 50, ncol = 20)
for(i in seq(ncol(x.mat))){
x.mat[,i] = runif(nrow(x.mat), -5, 5)
sd.temp = sd(x.mat[,i])
mean.temp = mean(x.mat[,i])
x.mat[,i] = (x.mat[,i] - mean.temp) / (2 * sd.temp)
}
y.vec = x.mat %*% true.beta.vec + true.intercept
sigma <- rnorm(50, y.vec, 0.5)
y.vec = y.vec + sigma
data = data.frame(cbind(x.mat, y.vec))
# prior guess for the number of relevant variables
p0 = 5
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
n_input = 20
n_sig_input = 5
set.seed(13)
true.beta.vec = NULL
sig.beta.idx <- sample(seq(n_input), n_sig_input)
true.beta.vec <- rnorm(n_input, 0.001, 0.01)
true.beta.vec[sig.beta.idx] <- rnorm(n_sig_input, 14, 3)
true.intercept <- rnorm(1, 0.5, 3)
x.mat = matrix(nrow = 50, ncol = 20)
for(i in seq(ncol(x.mat))){
x.mat[,i] = runif(nrow(x.mat), -5, 5)
sd.temp = sd(x.mat[,i])
mean.temp = mean(x.mat[,i])
x.mat[,i] = (x.mat[,i] - mean.temp) / (2 * sd.temp)
}
y.vec = x.mat %*% true.beta.vec + true.intercept
sigma <- rnorm(50, 0.1, 0.5)
y.vec = y.vec + sigma
data = data.frame(cbind(x.mat, y.vec))
# prior guess for the number of relevant variables
p0 = 5
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
var_selection <- varsel(fit, method='forward')
# variables ordered as they enter during the search
var_selection$vind
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.4)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(0, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(0, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection <- varsel(fit, method='forward')
# variables ordered as they enter during the search
var_selection$vind
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(0, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection <- varsel(fit, method='forward')
# variables ordered as they enter during the search
var_selection$vind
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
help("suggest_size")
# prior guess for the number of relevant variables
p0 = 2
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
var_selection <- varsel(fit, method='forward')
# variables ordered as they enter during the search
var_selection$vind
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
top_n = suggest_size(var_selection, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj <- project(var_selection, nv = top_n, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred <- proj_linpred(var_selection,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
# prior guess for the number of relevant variables
p0 = 2
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit2 = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
var_selection2 <- varsel(fit2, method='forward')
layout(c(3,2))
var_selection2 <- varsel(fit2, method='forward')
layout(3,2)
var_selection2 <- varsel(fit2, method='forward')
par(mfrow = c(3,2))
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(-2, 25))
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
ggplot() +
geom_point(aes(x=pred$pred2,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
par(mfrow = c(3,2))
varsel_plot(var_selection, stats=c('elpd', 'rmse'))
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj)) + coord_cartesian(xlim = c(-2, 25))
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
# prior guess for the number of relevant variables
p0 = 20
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit2 = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
# prior guess for the number of relevant variables
p0 = 18
n_obs = nrow(x.mat)
# scale for tau (notice that stan_glm will automatically scale this by sigma)
tau0 = p0/(n_input-p0) * 1/sqrt(n_obs)
# regularized horseshoe prior
prior_coeff = hs(global_scale = tau0, slab_scale = 1)
fit2 = stan_glm(y.vec ~ x.mat, family=gaussian(), data=data, prior=prior_coeff,
seed=13, chains=2, iter=500)
var_selection2 <- varsel(fit2, method='forward')
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = 1, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
var_selection2 <- varsel(fit2, method='forward')
varsel_plot(var_selection2, stats=c('elpd', 'rmse'))
top_n2 = suggest_size(var_selection2, alpha = 0.95, pct = 0.1)
# Estimates of top_n
proj2 <- project(var_selection2, nv = top_n2, ns = 500)
mcmc_areas(as.matrix(proj2)) + coord_cartesian(xlim = c(-2, 25))
par(mfrow=c(1,2))
### PREDICTION
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = top_n, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
pred2 <- proj_linpred(var_selection2,
xnew=x.mat,
ynew=y.vec,
nv = 2, integrated = TRUE)
ggplot() +
geom_point(aes(x=pred2$pred,y=y.vec)) +
geom_abline(slope = 1, color='red') +
labs(x = 'prediction', y = 'y')
1-pf(47.11,df1 = 3,df2=12)
1-pf(0.35487,df1 = 2,df2=6)
fake.data <- data.frame(   y =        c( 4,6,6,8,  8,8,6,6, 12,13,15,16),
grp = factor(c( 1,1,1,1,  2,2,2,2,   3,3,3,3 )) )
fake.data <- data.frame(   y =        c( 4,6,6,8,  8,8,6,6, 12,13,15,16),
grp = factor(c( 1,1,1,1,  2,2,2,2,   3,3,3,3 )) )
offset.representation <- lm(y ~ gap, data=fake.data)
fake.data <- data.frame(   y =        c( 4,6,6,8,  8,8,6,6, 12,13,15,16),
grp = factor(c( 1,1,1,1,  2,2,2,2,   3,3,3,3 )) )
offset.representation <- lm(y ~ grp, data=fake.data)
offset.representation
fake.data <- data.frame(   y =        c( 4,6,6,8,  8,8,6,6, 12,13,15,16),
grp = factor(c( 1,1,1,1,  2,2,2,2,   3,3,3,3 )) )
offset.representation <- lm(y ~ grp, data=fake.data)
anova(offset.representation)
data <- data('iris')
data
data('iris')
library(datasets)
data('iris')
iris
library(datasets)
data('iris')
force(iris)
library(datasets)
library(ggplot2)
data('iris')
ggplot(iris, aes(x=Species, y=Sepal.Length)) +
geom_boxplot()
model <- lm(Sepal.Length ~ Species, data=iris)
anova(model)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
library(emmeans)
install.packages("emmeans")
library(emmeans)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
天
t1$contrasts
library(emmeans)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
t1$contrasts['setosa - virginica',p.value]
library(emmeans)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
t1$contrasts['setosa - virginica','p.value']
library(emmeans)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
t1$contrasts
t1$contrasts['setosa - versicolor',]
t1$contrasts[2,]
library(emmeans)
model2 <- lm(Sepal.Length ~ Species, iris)
t1 <- emmeans::emmeans(model2, pairwise ~ Species)
t1$contrasts[2,]
1-pf(1-0.35487)
1-pf(1-0.35487,2,6)
0.5175/1.4583
1-pf(1-0.35487,2,6)
1-pf(0.35487,2,6)
1.035+8.7498
8.7498/6
install.packages("h5")
library(rstan)
library(h5)
install.packages("h5")
setwd("~/Documents/square-hinge-conv-interval - data Original/square-hinge-conv-interval/R-test")
library(penaltyLearning)
help("featureVector")
help("featureMatrix")
penaltyLearning::featureVector(data.vec = data)
data <- read.csv('data_label_F_processed.csv')
penaltyLearning::featureVector(data.vec = data)
data <- read.csv('data_label_F_processed.csv')[,(-2:-1)]
View(data)
data <- read.csv('data_label_F_processed.csv')[,(-3:-1)]
data <- read.csv('data_label_F_processed.csv')[,(-ncol(data)+1:-ncol(data))]
data <- read.csv('data_label_F_processed.csv')
data <- data[,(-ncol(data)+1:-ncol(data))]
data <- data[,-1]
penaltyLearning::featureVector(data.vec = data)
result = NULL
for(i in 1:nrow(data)){
result[i,] = penaltyLearning::featureVector(data.vec = data[i,])
}
data[i,]
penaltyLearning::featureVector(data.vec = data[i,])
length(data[i,])
is.numeric(data[i,])
is.numeric(as.numeric(data[i,]))
result = NULL
for(i in 1:nrow(data)){
result[i,] = penaltyLearning::featureVector(data.vec = is.numeric(data[i,]))
}
result = NULL
for(i in 1:nrow(data)){
result[i,] = penaltyLearning::featureVector(data.vec = as.numeric(data[i,]))
}
penaltyLearning::featureVector(data.vec = as.numeric(data[i,]))
result[i,] = penaltyLearning::featureVector(data.vec = as.numeric(data[i,]))
result[1,] = c(2,3,4)
length(penaltyLearning::featureVector(data.vec = as.numeric(data[i,])))
result = matrix(nrow = nrow(data), ncol = 365)
for(i in 1:nrow(data)){
result[i,] = penaltyLearning::featureVector(data.vec = as.numeric(data[i,]))
}
View(result)
write.csv(result, "./featured1000.csv")
data1 <- read.csv('featured100.csv')[,-1]
data1 <- read.csv('featured1000.csv')[,-1]
dd <- read.csv('data_label_F_processed.csv')
dd <- read.csv('data_label_F_processed.csv')[,-1]
data1 <- as.matrix(as.data.frame(lapply(data1, as.numeric)))
dd <- as.matrix(as.data.frame(lapply(dd, as.numeric)))
targets <- dd[,(ncol(dd)-2):(ncol(dd)-1)]
View(targets)
View(dd)
data <- read.csv('data_label_F_processed.csv')
View(data)
data <- read.csv('data_label_F_processed.csv')[,-1]
data <- data[,(-ncol(data)+8:-ncol(data))]
data <- read.csv('data_label_F_processed.csv')[,-1]
data <- data[,(-ncol(data)+7:-ncol(data))]
result = matrix(nrow = nrow(data), ncol = 365)
for(i in 1:nrow(data)){
result[i,] = penaltyLearning::featureVector(data.vec = as.numeric(data[i,]))
}
write.csv(result, "./featured1000.csv")
ncol(result)
data1 <- read.csv('featured1000.csv')[,-1]
data1 <- read.csv('featured1000.csv')[,-1]
dd <- read.csv('data_label_F_processed.csv')[,-1]
data1 <- as.matrix(as.data.frame(lapply(data1, as.numeric)))
dd <- as.matrix(as.data.frame(lapply(dd, as.numeric)))
View(dd)
targets <- dd[,(ncol(dd)-4):(ncol(dd)-3)]
View(targets)
folds <- as.integer(dd[,ncol(dd)])
tst_idx = 1
#data1 <- as.matrix(as.data.frame(lapply(data1, as.numeric)))
#targets <- as.matrix(as.data.frame(lapply(targets, as.numeric)))
data2 <- data1[,colSums(!is.na(data1)) > 0]
rowidx = ((targets[,1] == -Inf) & (targets[,2] == Inf))
data2 = data2[!rowidx,]
targets = targets[!rowidx,]
folds = folds[!rowidx]
fit.cv <- penaltyLearning::IntervalRegressionCV(feature.mat = data2,
target.mat = targets,
fold.vec = folds)
acc = NULL
for(idx in 1:6){
tst_idx = (folds == idx)
pred <- predict(fit.cv, data2[tst_idx,])
out <- targets[tst_idx,]
num = sum(ifelse(pred>out[,1],ifelse(pred<out[,2], 1, 0),0))
acc[idx] = num/nrow(out)
}
acc
